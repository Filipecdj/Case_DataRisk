{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"https://github.com/Filipecdj\" rel=\"some text\">\n",
    "  <img src=\"https://uploads-ssl.webflow.com/61954f44e9b26e303bba74f5/61a7850e3a3a86e7e5ba1f35_logo_datarisk_header.png\" height=\"100px\" style=\"display: block; margin: auto;\">\n",
    "</a>\n",
    "\n",
    "\n",
    "<h2 style=\"text-align:center\">Case Engenheiro de dados</h2>\n",
    "<p style=\"text-align:center;font-size:13px;\"></p>\n",
    "   \n",
    "<h2 style=\"text-align:center\">Filipe Carvalho de Jesus</h2>\n",
    "<p style=\"text-align:center;font-size:13px;\">Autor</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumário\n",
    "\n",
    "- [1. Questão 1](#1)<br>\n",
    "- [2. Questão 2](#2)<br>\n",
    "- [3. Questão 3](#3)<br>\n",
    "- [4. Questão 4](#4)<br>\n",
    "- [5. Questão 5](#5)<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Configurando ambiente PySpark na máquina <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalações\n",
    "!pip install pyspark -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extraindo informações diárias dos últimos 30 dias(úteis), através de um script Python.<a id=\"1\"></a>\n",
    "\n",
    "# Questão 1 - Extrair informações diárias de uma das ações abaixo dos últimos 30 dias(úteis):\n",
    "1. ITUB - Itaú\n",
    "2. BBD - Bradesco\n",
    "3. MSFT - Microsoft\n",
    "4. GOOG - Google\n",
    "5. TSLA - Tesla"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliotecas utilizadas. Como foi solicitado no desafio, foi armazenado o token de API em um arquivo chave_api.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from chave_api import chave\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciando o objeto para capturar a chave de api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chave_api = chave()\n",
    "key = chave_api.get_key()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listas criadas para parametrizar a captura de dados das respectivas ações: ITUB, BBD, MSFT, GOOG e TSLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acoes = ['ITUB','BBD','MSFT','GOOG','TSLA']\n",
    "lista_acoes = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Script abaixo, realiza a captura e a formatação dos dados em JSON das ações solicitadas pelo desafio. Onde é acessado a chave \"Time Series (Daily)\" do JSON, renomeia o index para \"Data\", seleciona as respectivas colunas, cria no dataframe uma colunas para indicar a ação, transforma o index em uma coluna, filtra os últimos 30 dias úteis e ao final todas elas são concatenadas em um dataframe Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for acao in acoes:\n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={acao}&outputsize=compact&apikey={key}'\n",
    "    r = requests.get(url)\n",
    "    data = r.json()\n",
    "\n",
    "    \n",
    "    time_series = data[\"Time Series (Daily)\"] \n",
    "\n",
    "    df = pd.DataFrame.from_dict(time_series, orient='index')\n",
    "    df.index.name = 'date'\n",
    "    df.columns = ['open', 'high', 'low', 'close', 'adjusted close', 'volume', 'dividend amount', 'split coefficient']\n",
    "    df = df.astype(float)\n",
    "    df['Symbol']= acao\n",
    "    df = df.reset_index()    \n",
    "    df = df.head(30)        \n",
    "    lista_acoes.append(df)\n",
    "\n",
    "df_acoes = pd.concat(lista_acoes, ignore_index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código em PySpark para realizar os questionários 2, 3, 4 e 5 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A configuração da sessão abaixo, está utilizando 3 threads juntamente com o arquivo .jar do PostreSQL. Isso, para realizar a conexão com o banco de dados Postgre local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[3]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>myApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x14cbe51e3b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".master(\"local[3]\")\\\n",
    ".appName(\"myApp\") \\\n",
    ".config(\"spark.jars\", \"\\postgresql-42.5.0.jar\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, é realizado a converção de um dataframe Pandas para um dataframe Pyspark, utilizando a função **createDataFrame** do PySpark. Após isso, é mostrado um exemplo do dataframe contendo as informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+------+-----+----------------+-----------+---------------+-----------------+------+\n",
      "|      date| open|  high|   low|close|  adjusted close|     volume|dividend amount|split coefficient|Symbol|\n",
      "+----------+-----+------+------+-----+----------------+-----------+---------------+-----------------+------+\n",
      "|2023-04-28| 5.09| 5.165|  5.06| 5.15|            5.15|1.7423588E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-27| 5.08|5.1475|  5.05| 5.14|            5.14|1.7235927E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-26| 5.02|5.0582|  4.97| 4.99|            4.99|1.2525425E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-25|  5.0|  5.05|  4.96| 5.02|            5.02|2.1134543E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-24| 4.99| 5.055|  4.92|  5.0|             5.0|1.6361481E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-21| 5.02|  5.03|  4.96| 5.03|            5.03|  5596926.0|            0.0|              1.0|  ITUB|\n",
      "|2023-04-20| 4.97|  5.06|  4.97| 5.02|            5.02|1.5589788E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-19| 5.09|  5.11| 4.994|  5.0|             5.0|1.9707169E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-18|5.155|  5.22|  5.11| 5.16|            5.16|2.0546294E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-17| 5.27|   5.3|  5.17| 5.21|            5.21|2.1892934E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-14| 5.12| 5.275|  5.11| 5.26|            5.26|2.6961901E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-13| 5.18|  5.26|  5.15| 5.16|            5.16|  3.14944E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-12|5.165|  5.27|  5.12|  5.2|             5.2|3.5669308E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-11| 4.97|5.0981|  4.96| 5.07|            5.07|4.4518724E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-10|  4.8| 4.855|4.7801| 4.83|            4.83|1.6985987E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-06| 4.83|  4.84| 4.745|  4.8|             4.8|1.9171148E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-05| 4.84|4.8899|  4.75| 4.82|            4.82|2.9240369E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-04| 4.78|  4.83| 4.745| 4.82|            4.82| 2.108554E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-03| 4.74| 4.775|  4.66| 4.71|            4.71|2.4841958E7|         0.0034|              1.0|  ITUB|\n",
      "|2023-03-31| 4.87|  4.92|   4.8| 4.87|4.86645399785125|7.1057738E7|            0.0|              1.0|  ITUB|\n",
      "+----------+-----+------+------+-----+----------------+-----------+---------------+-----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pysp_df = spark.createDataFrame(df_acoes)\n",
    "pysp_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 2 - Valores da média, o desvio padrão, o valor mínimo, os quartis da distribuição e o valor máximo dos últimos 30 dias(úteis).<a id=\"2\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtro para responder a questão 2, onde é mostrado: Média, desvio padrão, valor mínimo, quartis e valor máximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------+---------+------+------+------+---------+\n",
      "|Symbol|media_acao|desvio_padrao|valor_min|    q1|    q2|    q3|valor_max|\n",
      "+------+----------+-------------+---------+------+------+------+---------+\n",
      "|  ITUB|    4.8427|       0.2878|     4.33|  4.56|  4.83|  5.07|     5.26|\n",
      "|   BBD|    2.6347|       0.1272|     2.38|  2.52|  2.63|  2.76|     2.83|\n",
      "|  GOOG|   105.301|       2.1796|   101.32|104.22|105.12|106.42|   109.46|\n",
      "|  MSFT|  284.8203|       8.3014|   272.23|279.43|284.34|288.45|   307.26|\n",
      "|  TSLA|  182.4837|       13.217|   153.75|180.13|185.06|191.81|   207.46|\n",
      "+------+----------+-------------+---------+------+------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quest2 = pysp_df.groupBy(\"Symbol\").agg(\n",
    "    round(mean(\"close\"),4).alias(\"media_acao\"),\n",
    "    round(stddev(\"close\"),4).alias(\"desvio_padrao\"),\n",
    "    min(\"close\").alias(\"valor_min\"),\n",
    "    percentile_approx(\"close\", 0.25).alias(\"q1\"),\n",
    "    percentile_approx(\"close\", 0.5).alias(\"q2\"),\n",
    "    percentile_approx(\"close\", 0.75).alias(\"q3\"),\n",
    "    max(\"close\").alias(\"valor_max\"),\n",
    ")\n",
    "\n",
    "quest2.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 3 - Fazer uma ordenação dos dados e seleção dos 'n' maiores e menores volumes dos últimos 30 dias(úteis)<a id=\"3\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código abaixo, ordena o dataframe em ordem crescente com base no volume, selecionando os 2 maiores (df_maior) e menores (df_menor) volumes. É possivel alterar o valor de \"n\" para visualizar mais dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "quest3 = pysp_df.orderBy(\"volume\")\n",
    "\n",
    "n = 2\n",
    "df_maior = quest3.orderBy(desc(\"volume\")).limit(n)\n",
    "df_menor = quest3.limit(n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menores volumes comparando todas ações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+-----+-----+--------------+---------+---------------+-----------------+------+\n",
      "|      date|open|high|  low|close|adjusted close|   volume|dividend amount|split coefficient|Symbol|\n",
      "+----------+----+----+-----+-----+--------------+---------+---------------+-----------------+------+\n",
      "|2023-04-21|5.02|5.03| 4.96| 5.03|          5.03|5596926.0|            0.0|              1.0|  ITUB|\n",
      "|2023-04-21|2.68|2.69|2.655| 2.69|          2.69|8210718.0|            0.0|              1.0|   BBD|\n",
      "+----------+----+----+-----+-----+--------------+---------+---------------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_menor.show(n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maiores volumes comparando todas ações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+------+------+--------------+------------+---------------+-----------------+------+\n",
      "|      date|   open|  high|   low| close|adjusted close|      volume|dividend amount|split coefficient|Symbol|\n",
      "+----------+-------+------+------+------+--------------+------------+---------------+-----------------+------+\n",
      "|2023-04-20|166.165| 169.7|160.56|162.99|        162.99|2.10970819E8|            0.0|              1.0|  TSLA|\n",
      "|2023-03-31| 197.53|207.79| 197.2|207.46|        207.46|1.70222118E8|            0.0|              1.0|  TSLA|\n",
      "+----------+-------+------+------+------+--------------+------------+---------------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_maior.show(n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 4 - Faça a soma dos volumes de ITUB e BBD dos últimos 30 dias(úteis)<a id=\"4\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O seguinte código, filtra o dataframe para mostrar somente as ações ITUB e BBD e realiza a soma total dos volumes. Ao final, é printado o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soma do volume BBD e ITUB: 1591330590.0\n"
     ]
    }
   ],
   "source": [
    "quest4 = pysp_df.filter(pysp_df.Symbol.isin('BBD','ITUB'))\n",
    "soma_volumes = (quest4.select(sum('volume')).collect()[0][0])\n",
    "\n",
    "print(f'Soma do volume BBD e ITUB: {soma_volumes}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtra o dataframe da ação ITUB para somar o volume dos últimos 30 dias úteis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soma do volume ITUB: 811756436.0\n"
     ]
    }
   ],
   "source": [
    "soma_itub = pysp_df.filter(pysp_df.Symbol.isin('ITUB'))\n",
    "soma_volumes_itub = (soma_itub.select(sum('volume')).collect()[0][0])\n",
    "\n",
    "print(f'Soma do volume ITUB: {soma_volumes_itub}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtra o dataframe da ação BBD para somar o volume dos últimos 30 dias úteis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soma do volume BBD: 779574154.0\n"
     ]
    }
   ],
   "source": [
    "soma_bbd = pysp_df.filter(pysp_df.Symbol.isin('BBD'))\n",
    "soma_volumes_bbd = (soma_bbd.select(sum('volume')).collect()[0][0])\n",
    "\n",
    "print(f'Soma do volume BBD: {soma_volumes_bbd}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 5 - Faça a exportação do arquivo .csv pela API, crie uma tabela e a importe o .csv das ações de TSLA - Tesla em um banco PostsgreSQL.<a id=\"5\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código abaixo, salva o arquivo (TSLA.CSV) das ações TSLA, na pasta do projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=TSLA&apikey={key}&datatype=csv'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(r'TSLA.csv', 'w') as file:\n",
    "    file.write(response.text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Após isso, é executado dentro do banco de dados Postgre, a criação da tabela \"tsla\" com a seguinte query SQL:\n",
    " \n",
    " ```\n",
    " CREATE TABLE tsla(\n",
    "   timestamp Timestamp, \n",
    "   open float, \n",
    "   high float, \n",
    "   low float, \n",
    "   close float, \n",
    "   adjusted_close float, \n",
    "   volume float, \n",
    "   dividend_amount float, \n",
    "   split_coefficient float)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, o CSV exportado da API é transformado em um dataframe. Logo após, é criada a configuração de conexão para realizar a carga de dados no banco de dados PostgreSQL. Esse banco de dados foi configurado em um ambiente local, utilizando um container docker. O arquivo **docker-compose.yml** também está disponível na pasta do projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"TSLA.csv\", header=True, inferSchema=True)\n",
    "\n",
    "postgres_url = \"jdbc:postgresql://192.168.1.12:5432/datarisk_desafio\"\n",
    "postgres_user = \"admin\"\n",
    "postgres_password = \"admin\"\n",
    "postgres_table = \"tsla\"\n",
    "\n",
    "df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", postgres_url) \\\n",
    "    .option(\"dbtable\", postgres_table) \\\n",
    "    .option(\"user\", postgres_user) \\\n",
    "    .option(\"password\", postgres_password) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considerações\n",
    "\n",
    "### O desafio proposto não apresenta um alto nível de complexidade, no entanto, pude colocar em prática conhecimentos básicos que são fundamentais para o dia a dia de um engenheiro de dados. Mesmo sendo um desafio simples, foi uma oportunidade valiosa para aprimorar habilidades e ampliar o meu repertório na área.\n",
    "\n",
    "### Gostaria de expressar minha gratidão a toda equipe DataRisk pela oportunidade de participar do desafio e aplicar meus conhecimentos, muito obriogado.\n",
    "\n",
    "##### Filipe Carvalho de Jesus - filipe.cdj@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
