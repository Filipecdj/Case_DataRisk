{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condigurando ambiente PySpark na máquina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalações\n",
    "!pip install pyspark -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando ambiente\n",
    "spark = SparkSession.builder.master(\"local[3]\").getOrCreate() # \"*\" means the core quantity used\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extraindo informações diárias dos últimos 30 dias(úteis), através de um script Python.\n",
    "\n",
    "# Questão 1 - Extrair informações diárias de uma das ações abaixo dos últimos 30 dias(úteis):\n",
    "1. ITUB - Itaú\n",
    "2. BBD - Bradesco\n",
    "3. MSFT - Microsoft\n",
    "4. GOOG - Google\n",
    "5. TSLA - Tesla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Bibliotecas utilizadas. Como foi solicitado no desafio, foi armazenado o token de API em um arquivo chave_api.py\n",
    "import requests\n",
    "import pandas as pd\n",
    "from chave_api import chave\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chave_api = chave()\n",
    "key = chave_api.get_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Listas criadas para parametrizar a captura de dados das respectivas ações: ITUB, BBD, MSFT, GOOG e TSLA\n",
    "acoes = ['ITUB','BBD','MSFT','GOOG','TSLA']\n",
    "lista_acoes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Laço for para capturar o JSON das ações e concatenar em um dataframe\n",
    "for acao in acoes:\n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={acao}&outputsize=compact&apikey={key}'\n",
    "    r = requests.get(url)\n",
    "    data = r.json()\n",
    "\n",
    "    # acessa a chave \"Time Series (Daily)\" do dicionário\n",
    "    time_series = data[\"Time Series (Daily)\"]\n",
    "\n",
    "    df = pd.DataFrame.from_dict(time_series, orient='index')\n",
    "    df.index.name = 'date'\n",
    "    df.columns = ['open', 'high', 'low', 'close', 'adjusted close', 'volume', 'dividend amount', 'split coefficient']\n",
    "    df = df.astype(float)\n",
    "    df['Symbol']= acao\n",
    "    df = df.reset_index()    #Transforma o index em uma coluna\n",
    "    df = df.head(30)         #Filtra os últimos 30 dias úteis\n",
    "    lista_acoes.append(df)\n",
    "\n",
    "df_acoes = pd.concat(lista_acoes, ignore_index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código em PySpark para realizar os questionários 2, 3, 4 e 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-1D4CVAK.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[3]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>myApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a3c20101f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Criando uma sessão no PySpark\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local[3]\")\\\n",
    ".appName(\"myApp\") \\\n",
    ".config(\"spark.jars\", \"\\postgresql-42.5.0.jar\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+------+-----+----------------+-----------+---------------+-----------------+------+\n",
      "|      date| open|  high|   low|close|  adjusted close|     volume|dividend amount|split coefficient|Symbol|\n",
      "+----------+-----+------+------+-----+----------------+-----------+---------------+-----------------+------+\n",
      "|2023-04-28| 5.09| 5.165|  5.06| 5.15|            5.15|1.7423588E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-27| 5.08|5.1475|  5.05| 5.14|            5.14|1.7235927E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-26| 5.02|5.0582|  4.97| 4.99|            4.99|1.2525425E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-25|  5.0|  5.05|  4.96| 5.02|            5.02|2.1134543E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-24| 4.99| 5.055|  4.92|  5.0|             5.0|1.6361481E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-21| 5.02|  5.03|  4.96| 5.03|            5.03|  5596926.0|            0.0|              1.0|  ITUB|\n",
      "|2023-04-20| 4.97|  5.06|  4.97| 5.02|            5.02|1.5589788E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-19| 5.09|  5.11| 4.994|  5.0|             5.0|1.9707169E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-18|5.155|  5.22|  5.11| 5.16|            5.16|2.0546294E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-17| 5.27|   5.3|  5.17| 5.21|            5.21|2.1892934E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-14| 5.12| 5.275|  5.11| 5.26|            5.26|2.6961901E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-13| 5.18|  5.26|  5.15| 5.16|            5.16|  3.14944E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-12|5.165|  5.27|  5.12|  5.2|             5.2|3.5669308E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-11| 4.97|5.0981|  4.96| 5.07|            5.07|4.4518724E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-10|  4.8| 4.855|4.7801| 4.83|            4.83|1.6985987E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-06| 4.83|  4.84| 4.745|  4.8|             4.8|1.9171148E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-05| 4.84|4.8899|  4.75| 4.82|            4.82|2.9240369E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-04| 4.78|  4.83| 4.745| 4.82|            4.82| 2.108554E7|            0.0|              1.0|  ITUB|\n",
      "|2023-04-03| 4.74| 4.775|  4.66| 4.71|            4.71|2.4841958E7|         0.0034|              1.0|  ITUB|\n",
      "|2023-03-31| 4.87|  4.92|   4.8| 4.87|4.86645399785125|7.1057738E7|            0.0|              1.0|  ITUB|\n",
      "+----------+-----+------+------+-----+----------------+-----------+---------------+-----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Passando o dataframe em pandas para um dataframe PySpark\n",
    "pysp_df = spark.createDataFrame(df_acoes)\n",
    "pysp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 2 - Valores da média, o desvio padrão, o valor mínimo, os quartis da distribuição e o valor máximo dos últimos 30 dias(úteis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------+---------+------+------+------+---------+\n",
      "|Symbol|media_acao|desvio_padrao|valor_min|    q1|    q2|    q3|valor_max|\n",
      "+------+----------+-------------+---------+------+------+------+---------+\n",
      "|  ITUB|    4.8427|       0.2878|     4.33|  4.56|  4.83|  5.07|     5.26|\n",
      "|   BBD|    2.6347|       0.1272|     2.38|  2.52|  2.63|  2.76|     2.83|\n",
      "|  GOOG|   105.301|       2.1796|   101.32|104.22|105.12|106.42|   109.46|\n",
      "|  MSFT|  284.8203|       8.3014|   272.23|279.43|284.34|288.45|   307.26|\n",
      "|  TSLA|  182.4837|       13.217|   153.75|180.13|185.06|191.81|   207.46|\n",
      "+------+----------+-------------+---------+------+------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Média, desvio padrão, valor mínimo, quartis e valor máximo\n",
    "quest2 = pysp_df.groupBy(\"Symbol\").agg(\n",
    "    round(mean(\"close\"),4).alias(\"media_acao\"),\n",
    "    round(stddev(\"close\"),4).alias(\"desvio_padrao\"),\n",
    "    min(\"close\").alias(\"valor_min\"),\n",
    "    percentile_approx(\"close\", 0.25).alias(\"q1\"),\n",
    "    percentile_approx(\"close\", 0.5).alias(\"q2\"),\n",
    "    percentile_approx(\"close\", 0.75).alias(\"q3\"),\n",
    "    max(\"close\").alias(\"valor_max\"),\n",
    ")\n",
    "\n",
    "# Exibir o resultado\n",
    "quest2.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 3 - Fazer uma ordenação dos dados e seleção dos 'n' maiores e menores volumes dos últimos 30 dias(úteis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Ordena o dataframe em ordem crescente com base no volume\n",
    "quest3 = pysp_df.orderBy(\"volume\")\n",
    "\n",
    "#Selecionando os 2 maiores(df_maior) e menores(df_menor) volumes\n",
    "n = 2\n",
    "df_maior = quest3.orderBy(desc(\"volume\")).limit(n)\n",
    "df_menor = quest3.limit(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+-----+-----+--------------+---------+---------------+-----------------+------+\n",
      "|      date|open|high|  low|close|adjusted close|   volume|dividend amount|split coefficient|Symbol|\n",
      "+----------+----+----+-----+-----+--------------+---------+---------------+-----------------+------+\n",
      "|2023-04-21|5.02|5.03| 4.96| 5.03|          5.03|5596926.0|            0.0|              1.0|  ITUB|\n",
      "|2023-04-21|2.68|2.69|2.655| 2.69|          2.69|8210718.0|            0.0|              1.0|   BBD|\n",
      "+----------+----+----+-----+-----+--------------+---------+---------------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Menores volumes comparando todas ações\n",
    "df_menor.show(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+------+------+--------------+------------+---------------+-----------------+------+\n",
      "|      date|   open|  high|   low| close|adjusted close|      volume|dividend amount|split coefficient|Symbol|\n",
      "+----------+-------+------+------+------+--------------+------------+---------------+-----------------+------+\n",
      "|2023-04-20|166.165| 169.7|160.56|162.99|        162.99|2.10970819E8|            0.0|              1.0|  TSLA|\n",
      "|2023-03-31| 197.53|207.79| 197.2|207.46|        207.46|1.70222118E8|            0.0|              1.0|  TSLA|\n",
      "+----------+-------+------+------+------+--------------+------------+---------------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Maiores volumes comparando todas ações\n",
    "df_maior.show(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 4 - Faça a soma dos volumes de ITUB e BBD dos últimos 30 dias(úteis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soma do volume BBD e ITUB: 1591330553.0\n"
     ]
    }
   ],
   "source": [
    "#Filtra o dataframe para mostrar somente as ações ITUB e BBD\n",
    "quest4 = pysp_df.filter(pysp_df.Symbol.isin('BBD','ITUB'))\n",
    "\n",
    "#Realiza a soma total dos volumes de ITUB e BBD\n",
    "soma_volumes = (quest4.select(sum('volume')).collect()[0][0])\n",
    "\n",
    "print(f'Soma do volume BBD e ITUB: {soma_volumes}')\n",
    "#Resultado da Soma do Volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soma do volume ITUB: 811756433.0\n"
     ]
    }
   ],
   "source": [
    "#Filtra o dataframe da ação ITUB\n",
    "soma_itub = pysp_df.filter(pysp_df.Symbol.isin('ITUB'))\n",
    "soma_volumes_itub = (soma_itub.select(sum('volume')).collect()[0][0])\n",
    "\n",
    "print(f'Soma do volume ITUB: {soma_volumes_itub}')\n",
    "#Resultado da soma do volume da ação ITUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soma do volume BBD: 779574120.0\n"
     ]
    }
   ],
   "source": [
    "#Filtra o dataframe da ação BBD\n",
    "soma_bbd = pysp_df.filter(pysp_df.Symbol.isin('BBD'))\n",
    "soma_volumes_bbd = (soma_bbd.select(sum('volume')).collect()[0][0])\n",
    "\n",
    "print(f'Soma do volume BBD: {soma_volumes_bbd}')\n",
    "#Resultado da soma do volume da ação BBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 5 - Faça a exportação do arquivo .csv pela API, crie uma tabela e a importe o .csv das ações de TSLA - Tesla em um banco PostsgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Salva o CSV das ações da TSLA na pasta do arquivo\n",
    "url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=TSLA&apikey={key}&datatype=csv'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(r'TSLA.csv', 'w') as file:\n",
    "    file.write(response.text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Após isso, é criado dentro do banco de dados a tabela \"tsla\" com a seguinte query SQL\n",
    "- CREATE TABLE tsla(\n",
    "-    timestamp Timestamp, \n",
    "-    open float, \n",
    "-    high float, \n",
    "-    low float, \n",
    "-    close float, \n",
    "-    adjusted_close float, \n",
    "-    volume float, \n",
    "-    dividend_amount float, \n",
    "-    split_coefficient float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformando o CSV exportado da API em um dataframe e realizando a carga de dados ao BD Postgre de destino.\n",
    "\n",
    "df = spark.read.csv(\"TSLA.csv\", header=True, inferSchema=True)\n",
    "\n",
    "#Configurações de conexão\n",
    "postgres_url = \"jdbc:postgresql://192.168.1.12:5432/datarisk_desafio\"\n",
    "postgres_user = \"admin\"\n",
    "postgres_password = \"admin\"\n",
    "postgres_table = \"tsla\"\n",
    "\n",
    "df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", postgres_url) \\\n",
    "    .option(\"dbtable\", postgres_table) \\\n",
    "    .option(\"user\", postgres_user) \\\n",
    "    .option(\"password\", postgres_password) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
